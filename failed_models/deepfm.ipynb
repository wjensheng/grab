{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deepfm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "abZDq5HIjm0u",
        "colab_type": "code",
        "outputId": "138677df-31d4-4ea4-a750-7b97272e9fd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx1R-0y9jwFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.preprocessing import scale, minmax_scale\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore')\n",
        "\n",
        "# change to path\n",
        "PATH='/content/drive/My Drive/Colab Notebooks/grab/'\n",
        "os.chdir(PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o0-Xx27jxNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH_LABELS = PATH + 'safety/labels'\n",
        "PATH_FEATURES = PATH + 'safety/features'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djq864Wuj0MF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "de2e8e59-582e-4946-9848-0da1859fedf0"
      },
      "source": [
        "train_df = pd.read_pickle(f'{PATH_FEATURES}/agg_df_159.pkl')\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bookingID</th>\n",
              "      <th>Accuracy_mean</th>\n",
              "      <th>Accuracy_min</th>\n",
              "      <th>Accuracy_max</th>\n",
              "      <th>Accuracy_std</th>\n",
              "      <th>Accuracy_fo_mean</th>\n",
              "      <th>Accuracy_so_mean</th>\n",
              "      <th>Accuracy_fo_min</th>\n",
              "      <th>Accuracy_so_min</th>\n",
              "      <th>Accuracy_fo_max</th>\n",
              "      <th>Accuracy_so_max</th>\n",
              "      <th>Bearing_mean</th>\n",
              "      <th>Bearing_min</th>\n",
              "      <th>Bearing_max</th>\n",
              "      <th>Bearing_std</th>\n",
              "      <th>Bearing_fo_mean</th>\n",
              "      <th>Bearing_so_mean</th>\n",
              "      <th>Bearing_fo_min</th>\n",
              "      <th>Bearing_so_min</th>\n",
              "      <th>Bearing_fo_max</th>\n",
              "      <th>Bearing_so_max</th>\n",
              "      <th>acceleration_x_mean</th>\n",
              "      <th>acceleration_x_min</th>\n",
              "      <th>acceleration_x_max</th>\n",
              "      <th>acceleration_x_std</th>\n",
              "      <th>acceleration_x_fo_mean</th>\n",
              "      <th>acceleration_x_so_mean</th>\n",
              "      <th>acceleration_x_fo_min</th>\n",
              "      <th>acceleration_x_so_min</th>\n",
              "      <th>acceleration_x_fo_max</th>\n",
              "      <th>acceleration_x_so_max</th>\n",
              "      <th>acceleration_y_mean</th>\n",
              "      <th>acceleration_y_min</th>\n",
              "      <th>acceleration_y_max</th>\n",
              "      <th>acceleration_y_std</th>\n",
              "      <th>acceleration_y_fo_mean</th>\n",
              "      <th>acceleration_y_so_mean</th>\n",
              "      <th>acceleration_y_fo_min</th>\n",
              "      <th>acceleration_y_so_min</th>\n",
              "      <th>acceleration_y_fo_max</th>\n",
              "      <th>...</th>\n",
              "      <th>total_gyro_fo_max</th>\n",
              "      <th>total_gyro_so_max</th>\n",
              "      <th>roll_mean</th>\n",
              "      <th>roll_min</th>\n",
              "      <th>roll_max</th>\n",
              "      <th>roll_std</th>\n",
              "      <th>roll_fo_mean</th>\n",
              "      <th>roll_so_mean</th>\n",
              "      <th>roll_fo_min</th>\n",
              "      <th>roll_so_min</th>\n",
              "      <th>roll_fo_max</th>\n",
              "      <th>roll_so_max</th>\n",
              "      <th>pitch_mean</th>\n",
              "      <th>pitch_min</th>\n",
              "      <th>pitch_max</th>\n",
              "      <th>pitch_std</th>\n",
              "      <th>pitch_fo_mean</th>\n",
              "      <th>pitch_so_mean</th>\n",
              "      <th>pitch_fo_min</th>\n",
              "      <th>pitch_so_min</th>\n",
              "      <th>pitch_fo_max</th>\n",
              "      <th>pitch_so_max</th>\n",
              "      <th>horsepower_mean</th>\n",
              "      <th>horsepower_min</th>\n",
              "      <th>horsepower_max</th>\n",
              "      <th>horsepower_std</th>\n",
              "      <th>horsepower_fo_mean</th>\n",
              "      <th>horsepower_so_mean</th>\n",
              "      <th>horsepower_fo_min</th>\n",
              "      <th>horsepower_so_min</th>\n",
              "      <th>horsepower_fo_max</th>\n",
              "      <th>horsepower_so_max</th>\n",
              "      <th>vc</th>\n",
              "      <th>Speed_1</th>\n",
              "      <th>Speed_1_2</th>\n",
              "      <th>Speed_1_2_3</th>\n",
              "      <th>horsepower_1</th>\n",
              "      <th>horsepower_1_2</th>\n",
              "      <th>horsepower_1_2_3</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>10.165339</td>\n",
              "      <td>4.0</td>\n",
              "      <td>48.000</td>\n",
              "      <td>3.855898</td>\n",
              "      <td>-3.988036e-03</td>\n",
              "      <td>0.003992</td>\n",
              "      <td>-36.000</td>\n",
              "      <td>-68.000</td>\n",
              "      <td>32.000</td>\n",
              "      <td>32.000</td>\n",
              "      <td>176.526099</td>\n",
              "      <td>0.037464</td>\n",
              "      <td>359.979767</td>\n",
              "      <td>129.231351</td>\n",
              "      <td>-0.101603</td>\n",
              "      <td>-0.280284</td>\n",
              "      <td>-356.680084</td>\n",
              "      <td>-678.707878</td>\n",
              "      <td>357.773807</td>\n",
              "      <td>697.542567</td>\n",
              "      <td>-0.711264</td>\n",
              "      <td>-4.692294</td>\n",
              "      <td>4.782614</td>\n",
              "      <td>0.928022</td>\n",
              "      <td>-0.001451</td>\n",
              "      <td>-0.001960</td>\n",
              "      <td>-5.310925</td>\n",
              "      <td>-10.184308</td>\n",
              "      <td>4.873383</td>\n",
              "      <td>8.453427</td>\n",
              "      <td>-9.613822</td>\n",
              "      <td>-12.764703</td>\n",
              "      <td>-6.119916</td>\n",
              "      <td>0.639934</td>\n",
              "      <td>0.000324</td>\n",
              "      <td>0.000934</td>\n",
              "      <td>-3.953137</td>\n",
              "      <td>-7.856479</td>\n",
              "      <td>3.903342</td>\n",
              "      <td>...</td>\n",
              "      <td>0.660225</td>\n",
              "      <td>0.732735</td>\n",
              "      <td>-1.737242</td>\n",
              "      <td>-2.229817</td>\n",
              "      <td>-1.347602</td>\n",
              "      <td>0.116742</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>-0.470042</td>\n",
              "      <td>-0.940743</td>\n",
              "      <td>0.615583</td>\n",
              "      <td>1.047816</td>\n",
              "      <td>0.073598</td>\n",
              "      <td>-0.425732</td>\n",
              "      <td>0.447192</td>\n",
              "      <td>0.094192</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.000210</td>\n",
              "      <td>-0.441226</td>\n",
              "      <td>-0.853901</td>\n",
              "      <td>0.505558</td>\n",
              "      <td>0.918500</td>\n",
              "      <td>89.169004</td>\n",
              "      <td>-10.169692</td>\n",
              "      <td>252.090958</td>\n",
              "      <td>71.968693</td>\n",
              "      <td>8.359973e-02</td>\n",
              "      <td>-4.773332e-02</td>\n",
              "      <td>-232.745306</td>\n",
              "      <td>-460.166618</td>\n",
              "      <td>231.245321</td>\n",
              "      <td>400.555202</td>\n",
              "      <td>1004</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>10.590596</td>\n",
              "      <td>0.162426</td>\n",
              "      <td>-10.169692</td>\n",
              "      <td>131.702010</td>\n",
              "      <td>1.613122</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3.718763</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.709</td>\n",
              "      <td>0.597933</td>\n",
              "      <td>1.058824e-03</td>\n",
              "      <td>-0.001296</td>\n",
              "      <td>-4.414</td>\n",
              "      <td>-8.828</td>\n",
              "      <td>4.709</td>\n",
              "      <td>6.709</td>\n",
              "      <td>124.198590</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>337.000000</td>\n",
              "      <td>89.861236</td>\n",
              "      <td>0.038824</td>\n",
              "      <td>-0.047114</td>\n",
              "      <td>-306.000000</td>\n",
              "      <td>-559.000000</td>\n",
              "      <td>304.000000</td>\n",
              "      <td>610.000000</td>\n",
              "      <td>-0.525406</td>\n",
              "      <td>-5.352994</td>\n",
              "      <td>3.813341</td>\n",
              "      <td>0.744157</td>\n",
              "      <td>-0.000824</td>\n",
              "      <td>0.001334</td>\n",
              "      <td>-4.131128</td>\n",
              "      <td>-8.524900</td>\n",
              "      <td>5.112138</td>\n",
              "      <td>8.847361</td>\n",
              "      <td>9.532086</td>\n",
              "      <td>6.623425</td>\n",
              "      <td>12.536156</td>\n",
              "      <td>0.533915</td>\n",
              "      <td>-0.000785</td>\n",
              "      <td>0.000634</td>\n",
              "      <td>-3.323172</td>\n",
              "      <td>-6.294559</td>\n",
              "      <td>3.170564</td>\n",
              "      <td>...</td>\n",
              "      <td>0.641245</td>\n",
              "      <td>0.657876</td>\n",
              "      <td>1.797626</td>\n",
              "      <td>1.434715</td>\n",
              "      <td>2.137340</td>\n",
              "      <td>0.088049</td>\n",
              "      <td>-0.000096</td>\n",
              "      <td>-0.000285</td>\n",
              "      <td>-0.537245</td>\n",
              "      <td>-0.783427</td>\n",
              "      <td>0.523479</td>\n",
              "      <td>0.965273</td>\n",
              "      <td>0.053107</td>\n",
              "      <td>-0.421987</td>\n",
              "      <td>0.512452</td>\n",
              "      <td>0.074387</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>-0.000123</td>\n",
              "      <td>-0.488136</td>\n",
              "      <td>-0.832619</td>\n",
              "      <td>0.452367</td>\n",
              "      <td>0.931832</td>\n",
              "      <td>77.562771</td>\n",
              "      <td>-10.135306</td>\n",
              "      <td>233.266532</td>\n",
              "      <td>69.489840</td>\n",
              "      <td>1.848545e-01</td>\n",
              "      <td>-1.158458e-01</td>\n",
              "      <td>-225.490708</td>\n",
              "      <td>-453.960890</td>\n",
              "      <td>233.169493</td>\n",
              "      <td>411.608531</td>\n",
              "      <td>851</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-9.197917</td>\n",
              "      <td>-9.093537</td>\n",
              "      <td>-9.465303</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3.930626</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8.000</td>\n",
              "      <td>1.117354</td>\n",
              "      <td>4.639175e-03</td>\n",
              "      <td>0.001793</td>\n",
              "      <td>-4.000</td>\n",
              "      <td>-8.345</td>\n",
              "      <td>5.000</td>\n",
              "      <td>6.000</td>\n",
              "      <td>173.794872</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>354.000000</td>\n",
              "      <td>119.316520</td>\n",
              "      <td>1.010309</td>\n",
              "      <td>1.388601</td>\n",
              "      <td>-337.000000</td>\n",
              "      <td>-676.000000</td>\n",
              "      <td>339.000000</td>\n",
              "      <td>590.000000</td>\n",
              "      <td>0.306786</td>\n",
              "      <td>-2.971295</td>\n",
              "      <td>1.956122</td>\n",
              "      <td>0.756589</td>\n",
              "      <td>0.003345</td>\n",
              "      <td>0.018050</td>\n",
              "      <td>-3.644086</td>\n",
              "      <td>-5.013611</td>\n",
              "      <td>3.464516</td>\n",
              "      <td>7.108602</td>\n",
              "      <td>9.843183</td>\n",
              "      <td>7.941810</td>\n",
              "      <td>13.333716</td>\n",
              "      <td>0.505693</td>\n",
              "      <td>0.001592</td>\n",
              "      <td>0.003449</td>\n",
              "      <td>-3.490852</td>\n",
              "      <td>-6.522003</td>\n",
              "      <td>3.031151</td>\n",
              "      <td>...</td>\n",
              "      <td>0.407373</td>\n",
              "      <td>0.563150</td>\n",
              "      <td>1.556422</td>\n",
              "      <td>1.328230</td>\n",
              "      <td>1.891112</td>\n",
              "      <td>0.102860</td>\n",
              "      <td>-0.000391</td>\n",
              "      <td>-0.000621</td>\n",
              "      <td>-0.436176</td>\n",
              "      <td>-0.777169</td>\n",
              "      <td>0.478420</td>\n",
              "      <td>0.635099</td>\n",
              "      <td>-0.031534</td>\n",
              "      <td>-0.202880</td>\n",
              "      <td>0.260122</td>\n",
              "      <td>0.075406</td>\n",
              "      <td>-0.000304</td>\n",
              "      <td>-0.001784</td>\n",
              "      <td>-0.310418</td>\n",
              "      <td>-0.639641</td>\n",
              "      <td>0.329223</td>\n",
              "      <td>0.490599</td>\n",
              "      <td>31.450526</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>104.777363</td>\n",
              "      <td>29.000906</td>\n",
              "      <td>4.596758e-01</td>\n",
              "      <td>4.409695e-01</td>\n",
              "      <td>-93.238772</td>\n",
              "      <td>-186.477544</td>\n",
              "      <td>93.238772</td>\n",
              "      <td>173.380469</td>\n",
              "      <td>195</td>\n",
              "      <td>9.002391</td>\n",
              "      <td>9.257438</td>\n",
              "      <td>8.185837</td>\n",
              "      <td>92.667590</td>\n",
              "      <td>104.777363</td>\n",
              "      <td>83.435178</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>151.807013</td>\n",
              "      <td>2.271227</td>\n",
              "      <td>353.855377</td>\n",
              "      <td>71.273774</td>\n",
              "      <td>0.024552</td>\n",
              "      <td>0.006779</td>\n",
              "      <td>-300.273964</td>\n",
              "      <td>-508.312253</td>\n",
              "      <td>331.624398</td>\n",
              "      <td>594.888779</td>\n",
              "      <td>-0.365117</td>\n",
              "      <td>-2.866458</td>\n",
              "      <td>2.019635</td>\n",
              "      <td>0.527220</td>\n",
              "      <td>-0.001801</td>\n",
              "      <td>0.001724</td>\n",
              "      <td>-3.085678</td>\n",
              "      <td>-5.133127</td>\n",
              "      <td>3.158801</td>\n",
              "      <td>5.254550</td>\n",
              "      <td>-9.406439</td>\n",
              "      <td>-18.847833</td>\n",
              "      <td>-7.064984</td>\n",
              "      <td>0.598023</td>\n",
              "      <td>0.000358</td>\n",
              "      <td>-0.000093</td>\n",
              "      <td>-9.794467</td>\n",
              "      <td>-10.051071</td>\n",
              "      <td>9.601715</td>\n",
              "      <td>...</td>\n",
              "      <td>0.625971</td>\n",
              "      <td>0.967315</td>\n",
              "      <td>-1.841256</td>\n",
              "      <td>-2.107056</td>\n",
              "      <td>-1.537207</td>\n",
              "      <td>0.077592</td>\n",
              "      <td>-0.000184</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>-0.366293</td>\n",
              "      <td>-0.728194</td>\n",
              "      <td>0.371167</td>\n",
              "      <td>0.595339</td>\n",
              "      <td>0.037733</td>\n",
              "      <td>-0.188548</td>\n",
              "      <td>0.329725</td>\n",
              "      <td>0.054380</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>-0.000176</td>\n",
              "      <td>-0.325465</td>\n",
              "      <td>-0.635692</td>\n",
              "      <td>0.368603</td>\n",
              "      <td>0.512254</td>\n",
              "      <td>60.619916</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>385.485278</td>\n",
              "      <td>56.243974</td>\n",
              "      <td>-1.538936e-02</td>\n",
              "      <td>-1.304496e-02</td>\n",
              "      <td>-290.200024</td>\n",
              "      <td>-557.572728</td>\n",
              "      <td>267.372704</td>\n",
              "      <td>338.218808</td>\n",
              "      <td>1094</td>\n",
              "      <td>0.190000</td>\n",
              "      <td>5.600000</td>\n",
              "      <td>2.330000</td>\n",
              "      <td>1.842166</td>\n",
              "      <td>60.358278</td>\n",
              "      <td>22.639851</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4.586721</td>\n",
              "      <td>3.0</td>\n",
              "      <td>12.000</td>\n",
              "      <td>1.329545</td>\n",
              "      <td>-1.623727e-18</td>\n",
              "      <td>0.001917</td>\n",
              "      <td>-8.000</td>\n",
              "      <td>-15.720</td>\n",
              "      <td>9.000</td>\n",
              "      <td>11.225</td>\n",
              "      <td>197.812785</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>359.000000</td>\n",
              "      <td>111.868249</td>\n",
              "      <td>-0.256856</td>\n",
              "      <td>-0.169259</td>\n",
              "      <td>-328.000000</td>\n",
              "      <td>-640.000000</td>\n",
              "      <td>337.000000</td>\n",
              "      <td>602.000000</td>\n",
              "      <td>0.490616</td>\n",
              "      <td>-4.352792</td>\n",
              "      <td>5.593026</td>\n",
              "      <td>0.826271</td>\n",
              "      <td>-0.000269</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>-5.255433</td>\n",
              "      <td>-11.686455</td>\n",
              "      <td>6.431022</td>\n",
              "      <td>9.854836</td>\n",
              "      <td>9.538043</td>\n",
              "      <td>6.469330</td>\n",
              "      <td>13.249917</td>\n",
              "      <td>0.617210</td>\n",
              "      <td>0.000635</td>\n",
              "      <td>0.000416</td>\n",
              "      <td>-4.108576</td>\n",
              "      <td>-7.225922</td>\n",
              "      <td>3.265792</td>\n",
              "      <td>...</td>\n",
              "      <td>0.613616</td>\n",
              "      <td>0.840368</td>\n",
              "      <td>1.329015</td>\n",
              "      <td>0.903251</td>\n",
              "      <td>1.861424</td>\n",
              "      <td>0.097058</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>-0.498595</td>\n",
              "      <td>-1.169826</td>\n",
              "      <td>0.698923</td>\n",
              "      <td>1.004400</td>\n",
              "      <td>-0.050947</td>\n",
              "      <td>-0.577149</td>\n",
              "      <td>0.455444</td>\n",
              "      <td>0.084303</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>-0.657506</td>\n",
              "      <td>-1.022886</td>\n",
              "      <td>0.543940</td>\n",
              "      <td>1.201446</td>\n",
              "      <td>46.151636</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>212.361351</td>\n",
              "      <td>53.504948</td>\n",
              "      <td>1.298981e-17</td>\n",
              "      <td>-6.630865e-16</td>\n",
              "      <td>-212.361351</td>\n",
              "      <td>-424.722703</td>\n",
              "      <td>212.361351</td>\n",
              "      <td>379.798831</td>\n",
              "      <td>1095</td>\n",
              "      <td>4.268571</td>\n",
              "      <td>6.143076</td>\n",
              "      <td>5.507971</td>\n",
              "      <td>41.553927</td>\n",
              "      <td>61.591750</td>\n",
              "      <td>52.814251</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 159 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   bookingID  Accuracy_mean  ...  horsepower_1_2_3  label\n",
              "0          0      10.165339  ...          1.613122      0\n",
              "1          1       3.718763  ...         -9.465303      1\n",
              "2          2       3.930626  ...         83.435178      1\n",
              "3          4      10.000000  ...         22.639851      1\n",
              "4          6       4.586721  ...         52.814251      0\n",
              "\n",
              "[5 rows x 159 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngwx_u2HYLN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fake_test = train_df[-100:]\n",
        "\n",
        "fake_test.to_pickle(f'{PATH_FEATURES}/fake_test.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6TKTl9rgbjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://arxiv.org/abs/1703.04247"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unU6bIOwj1Hu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeatureDictionary(object):\n",
        "    def __init__(self, trainfile=None, testfile=None,\n",
        "                 dfTrain=None, dfTest=None, numeric_cols=[], ignore_cols=[]):\n",
        "        assert not ((trainfile is None) and (dfTrain is None)), \"trainfile or dfTrain at least one is set\"\n",
        "        assert not ((trainfile is not None) and (dfTrain is not None)), \"only one can be set\"\n",
        "        assert not ((testfile is None) and (dfTest is None)), \"testfile or dfTest at least one is set\"\n",
        "        assert not ((testfile is not None) and (dfTest is not None)), \"only one can be set\"\n",
        "        self.trainfile = trainfile\n",
        "        self.testfile = testfile\n",
        "        self.dfTrain = dfTrain\n",
        "        self.dfTest = dfTest\n",
        "        self.numeric_cols = numeric_cols\n",
        "        self.ignore_cols = ignore_cols\n",
        "        self.gen_feat_dict()\n",
        "\n",
        "    def gen_feat_dict(self):\n",
        "        if self.dfTrain is None:\n",
        "            dfTrain = pd.read_pkl(self.trainfile)\n",
        "        else:\n",
        "            dfTrain = self.dfTrain\n",
        "        if self.dfTest is None:\n",
        "            dfTest = pd.read_pkl(self.testfile)\n",
        "        else:\n",
        "            dfTest = self.dfTest\n",
        "        df = pd.concat([dfTrain, dfTest])\n",
        "        self.feat_dict = {}\n",
        "        tc = 0\n",
        "        for col in df.columns:\n",
        "            if col in self.numeric_cols:\n",
        "                # map to a single index\n",
        "                self.feat_dict[col] = tc\n",
        "                tc += 1\n",
        "            else:\n",
        "                us = df[col].unique()\n",
        "                self.feat_dict[col] = dict(zip(us, range(tc, len(us)+tc)))\n",
        "                tc += len(us)\n",
        "        self.feat_dim = tc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaZpda3Hj4Bo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataParser(object):\n",
        "    def __init__(self, feat_dict):\n",
        "        self.feat_dict = feat_dict\n",
        "\n",
        "    def parse(self, infile=None, df=None, has_label=False):\n",
        "        assert not ((infile is None) and (df is None)), \"infile or df at least one is set\"\n",
        "        assert not ((infile is not None) and (df is not None)), \"only one can be set\"\n",
        "        if infile is None:\n",
        "            dfi = df.copy()\n",
        "        else:\n",
        "            dfi = pd.read_pkl(infile)\n",
        "        if has_label:\n",
        "            y = dfi[\"label\"].values.tolist()\n",
        "            dfi.drop([\"bookingID\", \"label\"], axis=1, inplace=True)\n",
        "        else:\n",
        "            ids = dfi[\"bookingID\"].values.tolist()\n",
        "            dfi.drop([\"bookingID\"], axis=1, inplace=True)\n",
        "            \n",
        "        # dfi for feature index\n",
        "        # dfv for feature value which can be either binary (1/0) or float (e.g., 10.24)\n",
        "        \n",
        "        dfv = dfi.copy()\n",
        "        for col in dfi.columns:\n",
        "            if col in self.feat_dict.ignore_cols:\n",
        "                dfi.drop(col, axis=1, inplace=True)\n",
        "                dfv.drop(col, axis=1, inplace=True)\n",
        "                continue\n",
        "            if col in self.feat_dict.numeric_cols:\n",
        "                dfi[col] = self.feat_dict.feat_dict[col]\n",
        "            else:\n",
        "                dfi[col] = dfi[col].map(self.feat_dict.feat_dict[col])\n",
        "                dfv[col] = 1.\n",
        "\n",
        "        # list of list of feature indices of each sample in the dataset\n",
        "        Xi = dfi.values.tolist()\n",
        "        # list of list of feature values of each sample in the dataset\n",
        "        Xv = dfv.values.tolist()\n",
        "        if has_label:\n",
        "            return Xi, Xv, y\n",
        "        else:\n",
        "            return Xi, Xv, ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdbmIzVlj7MC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _load_data():\n",
        "    dfTrain = pd.read_pickle(f'{PATH_FEATURES}/agg_df_159.pkl')\n",
        "    dfTest = pd.read_pickle(f'{PATH_FEATURES}/fake_test.pkl')\n",
        "\n",
        "    cols = [c for c in train_df.columns if c not in ['bookingID', 'label']]\n",
        "    IGNORE_COLS = ['bookingID', 'label']\n",
        "    cols = [c for c in cols if c not in IGNORE_COLS]\n",
        "\n",
        "    X_train = dfTrain[cols].values\n",
        "    y_train = dfTrain[\"label\"].values\n",
        "    X_test = dfTest[cols].values\n",
        "    ids_test = dfTest[\"bookingID\"].values\n",
        "\n",
        "    return dfTrain, dfTest, X_train, y_train, X_test, ids_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WfCc2yOlTP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfTrain, dfTest, X_train, y_train, X_test, ids_test = _load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT_FLcBPo9Ai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folds = list(StratifiedKFold(n_splits=5, shuffle=True,\n",
        "                             random_state=4096).split(X_train, y_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKFug6_epU_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from time import time\n",
        "from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
        "\n",
        "\n",
        "class DeepFM(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, feature_size, field_size,\n",
        "                 embedding_size=8, dropout_fm=[1.0, 1.0],\n",
        "                 deep_layers=[32, 32], dropout_deep=[0.5, 0.5, 0.5],\n",
        "                 deep_layers_activation=tf.nn.relu,\n",
        "                 epoch=10, batch_size=256,\n",
        "                 learning_rate=0.001, optimizer_type=\"adam\",\n",
        "                 batch_norm=0, batch_norm_decay=0.995,\n",
        "                 verbose=False, random_seed=2016,\n",
        "                 use_fm=True, use_deep=True,\n",
        "                 loss_type=\"logloss\", eval_metric=roc_auc_score,\n",
        "                 l2_reg=0.0, greater_is_better=True):\n",
        "      \n",
        "        assert (use_fm or use_deep)\n",
        "        assert loss_type in [\"logloss\", \"mse\"], \\\n",
        "            \"loss_type can be either 'logloss' for classification task or 'mse' for regression task\"\n",
        "\n",
        "        self.feature_size = feature_size        # denote as M, size of the feature dictionary\n",
        "        self.field_size = field_size            # denote as F, size of the feature fields\n",
        "        self.embedding_size = embedding_size    # denote as K, size of the feature embedding\n",
        "\n",
        "        self.dropout_fm = dropout_fm\n",
        "        self.deep_layers = deep_layers\n",
        "        self.dropout_deep = dropout_deep\n",
        "        self.deep_layers_activation = deep_layers_activation\n",
        "        self.use_fm = use_fm\n",
        "        self.use_deep = use_deep\n",
        "        self.l2_reg = l2_reg\n",
        "\n",
        "        self.epoch = epoch\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizer_type = optimizer_type\n",
        "\n",
        "        self.batch_norm = batch_norm\n",
        "        self.batch_norm_decay = batch_norm_decay\n",
        "\n",
        "        self.verbose = verbose\n",
        "        self.random_seed = random_seed\n",
        "        self.loss_type = loss_type\n",
        "        self.eval_metric = eval_metric\n",
        "        self.greater_is_better = greater_is_better\n",
        "        self.train_result, self.valid_result = [], []\n",
        "\n",
        "        self._init_graph()\n",
        "\n",
        "\n",
        "    def _init_graph(self):\n",
        "        self.graph = tf.Graph()\n",
        "        with self.graph.as_default():\n",
        "\n",
        "            tf.set_random_seed(self.random_seed)\n",
        "\n",
        "            self.feat_index = tf.placeholder(tf.int32, shape=[None, None],\n",
        "                                                 name=\"feat_index\")  # None * F\n",
        "            self.feat_value = tf.placeholder(tf.float32, shape=[None, None],\n",
        "                                                 name=\"feat_value\")  # None * F\n",
        "            self.label = tf.placeholder(tf.float32, shape=[None, 1], name=\"label\")  # None * 1\n",
        "            self.dropout_keep_fm = tf.placeholder(tf.float32, shape=[None], name=\"dropout_keep_fm\")\n",
        "            self.dropout_keep_deep = tf.placeholder(tf.float32, shape=[None], name=\"dropout_keep_deep\")\n",
        "            self.train_phase = tf.placeholder(tf.bool, name=\"train_phase\")\n",
        "\n",
        "            self.weights = self._initialize_weights()\n",
        "\n",
        "            # model\n",
        "            self.embeddings = tf.nn.embedding_lookup(self.weights[\"feature_embeddings\"],\n",
        "                                                             self.feat_index)  # None * F * K\n",
        "            feat_value = tf.reshape(self.feat_value, shape=[-1, self.field_size, 1])\n",
        "            self.embeddings = tf.multiply(self.embeddings, feat_value)\n",
        "\n",
        "            # ---------- first order term ----------\n",
        "            self.y_first_order = tf.nn.embedding_lookup(self.weights[\"feature_bias\"], self.feat_index) # None * F * 1\n",
        "            self.y_first_order = tf.reduce_sum(tf.multiply(self.y_first_order, feat_value), 2)  # None * F\n",
        "            self.y_first_order = tf.nn.dropout(self.y_first_order, self.dropout_keep_fm[0]) # None * F\n",
        "\n",
        "            # ---------- second order term ---------------\n",
        "            # sum_square part\n",
        "            self.summed_features_emb = tf.reduce_sum(self.embeddings, 1)  # None * K\n",
        "            self.summed_features_emb_square = tf.square(self.summed_features_emb)  # None * K\n",
        "\n",
        "            # square_sum part\n",
        "            self.squared_features_emb = tf.square(self.embeddings)\n",
        "            self.squared_sum_features_emb = tf.reduce_sum(self.squared_features_emb, 1)  # None * K\n",
        "\n",
        "            # second order\n",
        "            self.y_second_order = 0.5 * tf.subtract(self.summed_features_emb_square, self.squared_sum_features_emb)  # None * K\n",
        "            self.y_second_order = tf.nn.dropout(self.y_second_order, self.dropout_keep_fm[1])  # None * K\n",
        "\n",
        "            # ---------- Deep component ----------\n",
        "            self.y_deep = tf.reshape(self.embeddings, shape=[-1, self.field_size * self.embedding_size]) # None * (F*K)\n",
        "            self.y_deep = tf.nn.dropout(self.y_deep, self.dropout_keep_deep[0])\n",
        "            for i in range(0, len(self.deep_layers)):\n",
        "                self.y_deep = tf.add(tf.matmul(self.y_deep, self.weights[\"layer_%d\" %i]), self.weights[\"bias_%d\"%i]) # None * layer[i] * 1\n",
        "                if self.batch_norm:\n",
        "                    self.y_deep = self.batch_norm_layer(self.y_deep, train_phase=self.train_phase, scope_bn=\"bn_%d\" %i) # None * layer[i] * 1\n",
        "                self.y_deep = self.deep_layers_activation(self.y_deep)\n",
        "                self.y_deep = tf.nn.dropout(self.y_deep, self.dropout_keep_deep[1+i]) # dropout at each Deep layer\n",
        "\n",
        "            # ---------- DeepFM ----------\n",
        "            if self.use_fm and self.use_deep:\n",
        "                concat_input = tf.concat([self.y_first_order, self.y_second_order, self.y_deep], axis=1)\n",
        "            elif self.use_fm:\n",
        "                concat_input = tf.concat([self.y_first_order, self.y_second_order], axis=1)\n",
        "            elif self.use_deep:\n",
        "                concat_input = self.y_deep\n",
        "            self.out = tf.add(tf.matmul(concat_input, self.weights[\"concat_projection\"]), self.weights[\"concat_bias\"])\n",
        "\n",
        "            # loss\n",
        "            if self.loss_type == \"logloss\":\n",
        "                self.out = tf.nn.sigmoid(self.out)\n",
        "                self.loss = tf.losses.log_loss(self.label, self.out)\n",
        "            elif self.loss_type == \"mse\":\n",
        "                self.loss = tf.nn.l2_loss(tf.subtract(self.label, self.out))\n",
        "            # l2 regularization on weights\n",
        "            if self.l2_reg > 0:\n",
        "                self.loss += tf.contrib.layers.l2_regularizer(\n",
        "                    self.l2_reg)(self.weights[\"concat_projection\"])\n",
        "                if self.use_deep:\n",
        "                    for i in range(len(self.deep_layers)):\n",
        "                        self.loss += tf.contrib.layers.l2_regularizer(\n",
        "                            self.l2_reg)(self.weights[\"layer_%d\"%i])\n",
        "\n",
        "            # optimizer\n",
        "            if self.optimizer_type == \"adam\":\n",
        "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999,\n",
        "                                                        epsilon=1e-8).minimize(self.loss)\n",
        "            elif self.optimizer_type == \"adagrad\":\n",
        "                self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate,\n",
        "                                                           initial_accumulator_value=1e-8).minimize(self.loss)\n",
        "            elif self.optimizer_type == \"gd\":\n",
        "                self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
        "            elif self.optimizer_type == \"momentum\":\n",
        "                self.optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=0.95).minimize(\n",
        "                    self.loss)\n",
        "\n",
        "            # init\n",
        "            self.saver = tf.train.Saver()\n",
        "            init = tf.global_variables_initializer()\n",
        "            self.sess = self._init_session()\n",
        "            self.sess.run(init)\n",
        "\n",
        "            # number of params\n",
        "            total_parameters = 0\n",
        "            for variable in self.weights.values():\n",
        "                shape = variable.get_shape()\n",
        "                variable_parameters = 1\n",
        "                for dim in shape:\n",
        "                    variable_parameters *= dim.value\n",
        "                total_parameters += variable_parameters\n",
        "            if self.verbose > 0:\n",
        "                print(\"# params: %d\" % total_parameters)\n",
        "\n",
        "\n",
        "    def _init_session(self):\n",
        "        config = tf.ConfigProto(device_count={\"gpu\": 0})\n",
        "        config.gpu_options.allow_growth = True\n",
        "        return tf.Session(config=config)\n",
        "\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        weights = dict()\n",
        "\n",
        "        # embeddings\n",
        "        weights[\"feature_embeddings\"] = tf.Variable(\n",
        "            tf.random_normal([self.feature_size, self.embedding_size], 0.0, 0.01),\n",
        "            name=\"feature_embeddings\")  # feature_size * K\n",
        "        weights[\"feature_bias\"] = tf.Variable(\n",
        "            tf.random_uniform([self.feature_size, 1], 0.0, 1.0), name=\"feature_bias\")  # feature_size * 1\n",
        "\n",
        "        # deep layers\n",
        "        num_layer = len(self.deep_layers)\n",
        "        input_size = self.field_size * self.embedding_size\n",
        "        glorot = np.sqrt(2.0 / (input_size + self.deep_layers[0]))\n",
        "        weights[\"layer_0\"] = tf.Variable(\n",
        "            np.random.normal(loc=0, scale=glorot, size=(input_size, self.deep_layers[0])), dtype=np.float32)\n",
        "        weights[\"bias_0\"] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[0])),\n",
        "                                                        dtype=np.float32)  # 1 * layers[0]\n",
        "        for i in range(1, num_layer):\n",
        "            glorot = np.sqrt(2.0 / (self.deep_layers[i-1] + self.deep_layers[i]))\n",
        "            weights[\"layer_%d\" % i] = tf.Variable(\n",
        "                np.random.normal(loc=0, scale=glorot, size=(self.deep_layers[i-1], self.deep_layers[i])),\n",
        "                dtype=np.float32)  # layers[i-1] * layers[i]\n",
        "            weights[\"bias_%d\" % i] = tf.Variable(\n",
        "                np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[i])),\n",
        "                dtype=np.float32)  # 1 * layer[i]\n",
        "\n",
        "        # final concat projection layer\n",
        "        if self.use_fm and self.use_deep:\n",
        "            input_size = self.field_size + self.embedding_size + self.deep_layers[-1]\n",
        "        elif self.use_fm:\n",
        "            input_size = self.field_size + self.embedding_size\n",
        "        elif self.use_deep:\n",
        "            input_size = self.deep_layers[-1]\n",
        "        glorot = np.sqrt(2.0 / (input_size + 1))\n",
        "        weights[\"concat_projection\"] = tf.Variable(\n",
        "                        np.random.normal(loc=0, scale=glorot, size=(input_size, 1)),\n",
        "                        dtype=np.float32)  # layers[i-1]*layers[i]\n",
        "        weights[\"concat_bias\"] = tf.Variable(tf.constant(0.01), dtype=np.float32)\n",
        "\n",
        "        return weights\n",
        "\n",
        "\n",
        "    def batch_norm_layer(self, x, train_phase, scope_bn):\n",
        "        bn_train = batch_norm(x, decay=self.batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
        "                              is_training=True, reuse=None, trainable=True, scope=scope_bn)\n",
        "        bn_inference = batch_norm(x, decay=self.batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
        "                                  is_training=False, reuse=True, trainable=True, scope=scope_bn)\n",
        "        z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n",
        "        return z\n",
        "\n",
        "\n",
        "    def get_batch(self, Xi, Xv, y, batch_size, index):\n",
        "        start = index * batch_size\n",
        "        end = (index+1) * batch_size\n",
        "        end = end if end < len(y) else len(y)\n",
        "        return Xi[start:end], Xv[start:end], [[y_] for y_ in y[start:end]]\n",
        "\n",
        "\n",
        "    # shuffle three lists simutaneously\n",
        "    def shuffle_in_unison_scary(self, a, b, c):\n",
        "        rng_state = np.random.get_state()\n",
        "        np.random.shuffle(a)\n",
        "        np.random.set_state(rng_state)\n",
        "        np.random.shuffle(b)\n",
        "        np.random.set_state(rng_state)\n",
        "        np.random.shuffle(c)\n",
        "\n",
        "\n",
        "    def fit_on_batch(self, Xi, Xv, y):\n",
        "        feed_dict = {self.feat_index: Xi,\n",
        "                     self.feat_value: Xv,\n",
        "                     self.label: y,\n",
        "                     self.dropout_keep_fm: self.dropout_fm,\n",
        "                     self.dropout_keep_deep: self.dropout_deep,\n",
        "                     self.train_phase: True}\n",
        "        loss, opt = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def fit(self, Xi_train, Xv_train, y_train,\n",
        "            Xi_valid=None, Xv_valid=None, y_valid=None,\n",
        "            early_stopping=False, refit=False):\n",
        "        \"\"\"\n",
        "        :param Xi_train: [[ind1_1, ind1_2, ...], [ind2_1, ind2_2, ...], ..., [indi_1, indi_2, ..., indi_j, ...], ...]\n",
        "                         indi_j is the feature index of feature field j of sample i in the training set\n",
        "        :param Xv_train: [[val1_1, val1_2, ...], [val2_1, val2_2, ...], ..., [vali_1, vali_2, ..., vali_j, ...], ...]\n",
        "                         vali_j is the feature value of feature field j of sample i in the training set\n",
        "                         vali_j can be either binary (1/0, for binary/categorical features) or float (e.g., 10.24, for numerical features)\n",
        "        :param y_train: label of each sample in the training set\n",
        "        :param Xi_valid: list of list of feature indices of each sample in the validation set\n",
        "        :param Xv_valid: list of list of feature values of each sample in the validation set\n",
        "        :param y_valid: label of each sample in the validation set\n",
        "        :param early_stopping: perform early stopping or not\n",
        "        :param refit: refit the model on the train+valid dataset or not\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        has_valid = Xv_valid is not None\n",
        "        for epoch in range(self.epoch):\n",
        "            t1 = time()\n",
        "            self.shuffle_in_unison_scary(Xi_train, Xv_train, y_train)\n",
        "            total_batch = int(len(y_train) / self.batch_size)\n",
        "            for i in range(total_batch):\n",
        "                Xi_batch, Xv_batch, y_batch = self.get_batch(Xi_train, Xv_train, y_train, self.batch_size, i)\n",
        "                self.fit_on_batch(Xi_batch, Xv_batch, y_batch)\n",
        "\n",
        "            # evaluate training and validation datasets\n",
        "            train_result = self.evaluate(Xi_train, Xv_train, y_train)\n",
        "            self.train_result.append(train_result)\n",
        "            if has_valid:\n",
        "                valid_result = self.evaluate(Xi_valid, Xv_valid, y_valid)\n",
        "                self.valid_result.append(valid_result)\n",
        "            if self.verbose > 0 and epoch % self.verbose == 0:\n",
        "                if has_valid:\n",
        "                    print(\"[%d] train-result=%.4f, valid-result=%.4f [%.1f s]\"\n",
        "                        % (epoch + 1, train_result, valid_result, time() - t1))\n",
        "                else:\n",
        "                    print(\"[%d] train-result=%.4f [%.1f s]\"\n",
        "                        % (epoch + 1, train_result, time() - t1))\n",
        "            if has_valid and early_stopping and self.training_termination(self.valid_result):\n",
        "                break\n",
        "\n",
        "        # fit a few more epoch on train+valid until result reaches the best_train_score\n",
        "        if has_valid and refit:\n",
        "            if self.greater_is_better:\n",
        "                best_valid_score = max(self.valid_result)\n",
        "            else:\n",
        "                best_valid_score = min(self.valid_result)\n",
        "            best_epoch = self.valid_result.index(best_valid_score)\n",
        "            best_train_score = self.train_result[best_epoch]\n",
        "            Xi_train = Xi_train + Xi_valid\n",
        "            Xv_train = Xv_train + Xv_valid\n",
        "            y_train = y_train + y_valid\n",
        "            for epoch in range(100):\n",
        "                self.shuffle_in_unison_scary(Xi_train, Xv_train, y_train)\n",
        "                total_batch = int(len(y_train) / self.batch_size)\n",
        "                for i in range(total_batch):\n",
        "                    Xi_batch, Xv_batch, y_batch = self.get_batch(Xi_train, Xv_train, y_train,\n",
        "                                                                self.batch_size, i)\n",
        "                    self.fit_on_batch(Xi_batch, Xv_batch, y_batch)\n",
        "                # check\n",
        "                train_result = self.evaluate(Xi_train, Xv_train, y_train)\n",
        "                if abs(train_result - best_train_score) < 0.001 or \\\n",
        "                    (self.greater_is_better and train_result > best_train_score) or \\\n",
        "                    ((not self.greater_is_better) and train_result < best_train_score):\n",
        "                    break\n",
        "\n",
        "\n",
        "    def training_termination(self, valid_result):\n",
        "        if len(valid_result) > 5:\n",
        "            if self.greater_is_better:\n",
        "                if valid_result[-1] < valid_result[-2] and \\\n",
        "                    valid_result[-2] < valid_result[-3] and \\\n",
        "                    valid_result[-3] < valid_result[-4] and \\\n",
        "                    valid_result[-4] < valid_result[-5]:\n",
        "                    return True\n",
        "            else:\n",
        "                if valid_result[-1] > valid_result[-2] and \\\n",
        "                    valid_result[-2] > valid_result[-3] and \\\n",
        "                    valid_result[-3] > valid_result[-4] and \\\n",
        "                    valid_result[-4] > valid_result[-5]:\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "\n",
        "    def predict(self, Xi, Xv):\n",
        "        \"\"\"\n",
        "        :param Xi: list of list of feature indices of each sample in the dataset\n",
        "        :param Xv: list of list of feature values of each sample in the dataset\n",
        "        :return: predicted probability of each sample\n",
        "        \"\"\"\n",
        "        # dummy y\n",
        "        dummy_y = [1] * len(Xi)\n",
        "        batch_index = 0\n",
        "        Xi_batch, Xv_batch, y_batch = self.get_batch(Xi, Xv, dummy_y, self.batch_size, batch_index)\n",
        "        y_pred = None\n",
        "        while len(Xi_batch) > 0:\n",
        "            num_batch = len(y_batch)\n",
        "            feed_dict = {self.feat_index: Xi_batch,\n",
        "                         self.feat_value: Xv_batch,\n",
        "                         self.label: y_batch,\n",
        "                         self.dropout_keep_fm: [1.0] * len(self.dropout_fm),\n",
        "                         self.dropout_keep_deep: [1.0] * len(self.dropout_deep),\n",
        "                         self.train_phase: False}\n",
        "            batch_out = self.sess.run(self.out, feed_dict=feed_dict)\n",
        "\n",
        "            if batch_index == 0:\n",
        "                y_pred = np.reshape(batch_out, (num_batch,))\n",
        "            else:\n",
        "                y_pred = np.concatenate((y_pred, np.reshape(batch_out, (num_batch,))))\n",
        "\n",
        "            batch_index += 1\n",
        "            Xi_batch, Xv_batch, y_batch = self.get_batch(Xi, Xv, dummy_y, self.batch_size, batch_index)\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "    def evaluate(self, Xi, Xv, y):\n",
        "        \"\"\"\n",
        "        :param Xi: list of list of feature indices of each sample in the dataset\n",
        "        :param Xv: list of list of feature values of each sample in the dataset\n",
        "        :param y: label of each sample in the dataset\n",
        "        :return: metric of the evaluation\n",
        "        \"\"\"\n",
        "        y_pred = self.predict(Xi, Xv)        \n",
        "        return self.eval_metric(y, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGLMsGXybMOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUMERIC_COLS = [c for c in train_df.columns if c not in ['bookingID', 'label'] and 'mean' in c]\n",
        "IGNORE_COLS = ['bookingID', 'label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCAJmBWLntOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _run_base_model_dfm(dfTrain, dfTest, folds, dfm_params):       \n",
        "    fd = FeatureDictionary(dfTrain=dfTrain, dfTest=dfTest,\n",
        "                           numeric_cols=NUMERIC_COLS,\n",
        "                           ignore_cols=IGNORE_COLS)\n",
        "    \n",
        "    data_parser = DataParser(feat_dict=fd)\n",
        "    Xi_train, Xv_train, y_train = data_parser.parse(df=dfTrain, has_label=True)\n",
        "    Xi_test, Xv_test, ids_test = data_parser.parse(df=dfTest)\n",
        "\n",
        "    dfm_params[\"feature_size\"] = fd.feat_dim\n",
        "    dfm_params[\"field_size\"] = len(Xi_train[0])\n",
        "\n",
        "    y_train_meta = np.zeros((dfTrain.shape[0], 1), dtype=float)\n",
        "    y_test_meta = np.zeros((dfTest.shape[0], 1), dtype=float)\n",
        "    \n",
        "    _get = lambda x, l: [x[i] for i in l]\n",
        "    \n",
        "    auc_cv = np.zeros(len(folds), dtype=float)\n",
        "    auc_epoch_train = np.zeros((len(folds), dfm_params[\"epoch\"]), dtype=float)\n",
        "    auc_epoch_valid = np.zeros((len(folds), dfm_params[\"epoch\"]), dtype=float)\n",
        "    \n",
        "    for i, (train_idx, valid_idx) in enumerate(folds):\n",
        "        Xi_train_, Xv_train_, y_train_ = _get(Xi_train, train_idx), _get(Xv_train, train_idx), _get(y_train, train_idx)\n",
        "        Xi_valid_, Xv_valid_, y_valid_ = _get(Xi_train, valid_idx), _get(Xv_train, valid_idx), _get(y_train, valid_idx)\n",
        "\n",
        "        dfm = DeepFM(**dfm_params)\n",
        "        dfm.fit(Xi_train_, Xv_train_, y_train_, Xi_valid_, Xv_valid_, y_valid_)\n",
        "\n",
        "        y_train_meta[valid_idx,0] = dfm.predict(Xi_valid_, Xv_valid_)\n",
        "        y_test_meta[:,0] += dfm.predict(Xi_test, Xv_test)\n",
        "\n",
        "        auc_cv[i] = roc_auc_score(y_valid_, y_train_meta[valid_idx])\n",
        "        auc_epoch_train[i] = dfm.train_result\n",
        "        auc_epoch_valid[i] = dfm.valid_result\n",
        "\n",
        "    y_test_meta /= float(len(folds))\n",
        "\n",
        "    # save result\n",
        "    if dfm_params[\"use_fm\"] and dfm_params[\"use_deep\"]:\n",
        "        clf_str = \"DeepFM\"\n",
        "    elif dfm_params[\"use_fm\"]:\n",
        "        clf_str = \"FM\"\n",
        "    elif dfm_params[\"use_deep\"]:\n",
        "        clf_str = \"DNN\"\n",
        "    print(\"%s: %.5f (%.5f)\"%(clf_str, auc_cv.mean(), auc_cv.std()))\n",
        "    filename = \"%s_Mean%.5f_Std%.5f.csv\"%(clf_str, auc_cv.mean(), auc_cv.std())\n",
        "\n",
        "    return y_train_meta, y_test_meta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eqbgzpUogVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfm_params = {\n",
        "    \"use_fm\": True,\n",
        "    \"use_deep\": True,\n",
        "    \"embedding_size\": 4,\n",
        "    \"dropout_fm\": [1.0, 1.0],\n",
        "    \"deep_layers\": [32, 32],\n",
        "    \"dropout_deep\": [0.8, 0.8, 0.8],\n",
        "    \"deep_layers_activation\": tf.nn.relu,\n",
        "    \"epoch\": 10,\n",
        "    \"batch_size\": 512,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"optimizer_type\": \"adam\",\n",
        "    \"batch_norm\": 1,\n",
        "    \"batch_norm_decay\": 0.995,\n",
        "    \"l2_reg\": 0.5,\n",
        "    \"verbose\": True,\n",
        "    \"random_seed\": 42\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yAtD19lqtJv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1019
        },
        "outputId": "3763d52f-8d19-4dd7-9645-d906467ae2ef"
      },
      "source": [
        "%%time\n",
        "y_train_dfm, y_test_dfm = _run_base_model_dfm(dfTrain, dfTest, folds, dfm_params)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#params: 9299913\n",
            "[1] train-result=0.6735, valid-result=0.5939 [5.6 s]\n",
            "[2] train-result=0.9986, valid-result=0.6185 [5.3 s]\n",
            "[3] train-result=0.9995, valid-result=0.6008 [5.2 s]\n",
            "[4] train-result=0.9995, valid-result=0.6530 [5.2 s]\n",
            "[5] train-result=0.9995, valid-result=0.6126 [5.2 s]\n",
            "[6] train-result=0.9998, valid-result=0.6444 [5.2 s]\n",
            "[7] train-result=0.9995, valid-result=0.6322 [5.2 s]\n",
            "[8] train-result=0.9998, valid-result=0.6511 [5.2 s]\n",
            "[9] train-result=0.9998, valid-result=0.6430 [5.2 s]\n",
            "[10] train-result=0.9998, valid-result=0.6376 [5.2 s]\n",
            "#params: 9299913\n",
            "[1] train-result=0.9886, valid-result=0.6053 [5.6 s]\n",
            "[2] train-result=0.9997, valid-result=0.6432 [5.2 s]\n",
            "[3] train-result=0.9998, valid-result=0.6259 [5.2 s]\n",
            "[4] train-result=0.9998, valid-result=0.5811 [5.2 s]\n",
            "[5] train-result=0.9998, valid-result=0.5628 [5.2 s]\n",
            "[6] train-result=0.9998, valid-result=0.5529 [5.2 s]\n",
            "[7] train-result=0.9999, valid-result=0.5436 [5.2 s]\n",
            "[8] train-result=0.9999, valid-result=0.5341 [5.2 s]\n",
            "[9] train-result=0.9999, valid-result=0.5252 [5.2 s]\n",
            "[10] train-result=0.9999, valid-result=0.5186 [5.2 s]\n",
            "#params: 9299913\n",
            "[1] train-result=0.3464, valid-result=0.3435 [5.8 s]\n",
            "[2] train-result=0.6714, valid-result=0.4059 [5.4 s]\n",
            "[3] train-result=0.9984, valid-result=0.6873 [5.4 s]\n",
            "[4] train-result=0.9996, valid-result=0.6883 [5.3 s]\n",
            "[5] train-result=0.9996, valid-result=0.6437 [5.3 s]\n",
            "[6] train-result=0.9996, valid-result=0.6278 [5.3 s]\n",
            "[7] train-result=0.9996, valid-result=0.6332 [5.3 s]\n",
            "[8] train-result=0.9996, valid-result=0.6351 [5.3 s]\n",
            "[9] train-result=0.9996, valid-result=0.6351 [5.3 s]\n",
            "[10] train-result=0.9996, valid-result=0.6350 [5.4 s]\n",
            "#params: 9299913\n",
            "[1] train-result=0.4812, valid-result=0.4063 [5.5 s]\n",
            "[2] train-result=0.9579, valid-result=0.6813 [5.2 s]\n",
            "[3] train-result=0.9998, valid-result=0.6608 [5.4 s]\n",
            "[4] train-result=0.9997, valid-result=0.6781 [5.4 s]\n",
            "[5] train-result=0.9996, valid-result=0.6530 [5.3 s]\n",
            "[6] train-result=0.9998, valid-result=0.6711 [5.1 s]\n",
            "[7] train-result=0.9998, valid-result=0.6768 [5.2 s]\n",
            "[8] train-result=0.9998, valid-result=0.6801 [5.2 s]\n",
            "[9] train-result=0.9998, valid-result=0.6827 [5.2 s]\n",
            "[10] train-result=0.9998, valid-result=0.6846 [5.2 s]\n",
            "#params: 9299913\n",
            "[1] train-result=0.8005, valid-result=0.5154 [5.7 s]\n",
            "[2] train-result=0.9979, valid-result=0.6295 [5.3 s]\n",
            "[3] train-result=0.9993, valid-result=0.6219 [5.4 s]\n",
            "[4] train-result=0.9996, valid-result=0.6450 [5.3 s]\n",
            "[5] train-result=0.9996, valid-result=0.6479 [5.4 s]\n",
            "[6] train-result=0.9996, valid-result=0.6491 [5.3 s]\n",
            "[7] train-result=0.9996, valid-result=0.6500 [5.4 s]\n",
            "[8] train-result=0.9996, valid-result=0.6509 [5.4 s]\n",
            "[9] train-result=0.9996, valid-result=0.6510 [5.5 s]\n",
            "[10] train-result=0.9996, valid-result=0.6514 [5.7 s]\n",
            "DeepFM: 0.62545 (0.05626)\n",
            "CPU times: user 8min 8s, sys: 7.26 s, total: 8min 15s\n",
            "Wall time: 4min 47s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9rc54TxiYxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}